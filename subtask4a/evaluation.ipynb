{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Iqg2UClTjpjO",
    "outputId": "736cefea-72de-44c4-c9a3-c366beea911c"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "32MAvakTjskM"
   },
   "outputs": [],
   "source": [
    "\n",
    "gold = pd.read_csv(\"data/ct_dev.tsv\", sep=\"\\t\")\n",
    "gold[\"labels\"] = gold[\"labels\"].apply(eval)\n",
    "\n",
    "# Unpack true labels\n",
    "gold[\"cat1_true\"] = gold[\"labels\"].apply(lambda x: int(x[0]))\n",
    "gold[\"cat2_true\"] = gold[\"labels\"].apply(lambda x: int(x[1]))\n",
    "gold[\"cat3_true\"] = gold[\"labels\"].apply(lambda x: int(x[2]))\n",
    "\n",
    "preds_bert = pd.read_csv(\"output/predictions_debert.csv\")\n",
    "preds_llama = pd.read_csv(\"output/predictions_llama.csv\")\n",
    "\n",
    "# Merge with gold by 'index'\n",
    "df_bert = pd.merge(gold, preds_bert, on=\"index\")\n",
    "df_llama = pd.merge(gold, preds_llama, on=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iJ2nrMnNkkVB"
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(df, model_name=\"Model\"):\n",
    "    metrics = {}\n",
    "    for i, cat in enumerate([\"cat1\", \"cat2\", \"cat3\"]):\n",
    "        y_true = df[f\"{cat}_true\"]\n",
    "        y_pred = df[f\"{cat}_pred\"]\n",
    "        \n",
    "        metrics[f\"{cat}_acc\"] = accuracy_score(y_true, y_pred)\n",
    "        metrics[f\"{cat}_prec\"] = precision_score(y_true, y_pred)\n",
    "        metrics[f\"{cat}_rec\"] = recall_score(y_true, y_pred)\n",
    "        metrics[f\"{cat}_f1\"] = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Macro F1 across all 3 categories\n",
    "    macro_f1 = f1_score(\n",
    "        df[[\"cat1_true\", \"cat2_true\", \"cat3_true\"]].values,\n",
    "        df[[\"cat1_pred\", \"cat2_pred\", \"cat3_pred\"]].values,\n",
    "        average=\"macro\"\n",
    "    )\n",
    "    metrics[\"macro_f1\"] = macro_f1\n",
    "    print(f\"\\n {model_name} Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DeBERTa Metrics:\n",
      "cat1_acc: 0.9197\n",
      "cat1_prec: 0.7419\n",
      "cat1_rec: 0.8846\n",
      "cat1_f1: 0.8070\n",
      "cat2_acc: 0.9197\n",
      "cat2_prec: 0.7778\n",
      "cat2_rec: 0.8077\n",
      "cat2_f1: 0.7925\n",
      "cat3_acc: 0.9197\n",
      "cat3_prec: 0.8108\n",
      "cat3_rec: 0.8824\n",
      "cat3_f1: 0.8451\n",
      "macro_f1: 0.8148\n",
      "\n",
      " LLaMA Metrics:\n",
      "cat1_acc: 0.9197\n",
      "cat1_prec: 0.7586\n",
      "cat1_rec: 0.8462\n",
      "cat1_f1: 0.8000\n",
      "cat2_acc: 0.9197\n",
      "cat2_prec: 0.7778\n",
      "cat2_rec: 0.8077\n",
      "cat2_f1: 0.7925\n",
      "cat3_acc: 0.9197\n",
      "cat3_prec: 0.8108\n",
      "cat3_rec: 0.8824\n",
      "cat3_f1: 0.8451\n",
      "macro_f1: 0.8125\n"
     ]
    }
   ],
   "source": [
    "bert_metrics = evaluate_predictions(df_bert, model_name=\"DeBERTa\")\n",
    "llama_metrics = evaluate_predictions(df_llama, model_name=\"LLaMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and tokenize test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05afd8bc66a54cd19b8e833fa118556f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Load and tokenize the test set (without labels)\n",
    "print(\"Load and tokenize test data\")\n",
    "test_data = pd.read_csv(\"data/ct_test.tsv\", sep='\\t')  # Adjust file path if needed\n",
    "\n",
    "# Add dummy labels for compatibility\n",
    "test_data[\"labels\"] = [[0.0, 0.0, 0.0]] * len(test_data)\n",
    "\n",
    "# Save to a temporary file so the DataLoader can read it with labels\n",
    "temp_test_path = \"temp_ct_test_with_labels.tsv\"\n",
    "test_data.to_csv(temp_test_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Reuse the data loader\n",
    "test_ds, _ = dl.get_dataset(temp_test_path)\n",
    "\n",
    "# Predict on the test set\n",
    "print(\"Predicting on test set\")\n",
    "test_pred_output = trainer.predict(test_ds)\n",
    "\n",
    "# Annotate test predictions\n",
    "test_df = annotate_test_dataframe(test_data, test_pred_output)\n",
    "\n",
    "# Save predictions\n",
    "submission_test_df = test_df[[\"index\", \"cat1_pred\", \"cat2_pred\", \"cat3_pred\"]]\n",
    "submission_test_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Test predictions saved to test_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
