{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4239d874",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05915dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/at043650/Desktop/github-private/paperQuest/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/at043650/Desktop/github-private/paperQuest/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from paper_quest import evaluate_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e271d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = {\n",
    "    \"cache_dir\": \"cache_temp\",\n",
    "    \"embedding_model\": \"sentence-transformers/allenai-specter\",\n",
    "    \"langchain_embedding\": \"nomic-embed-text\",\n",
    "    \"reranker_model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    \"candidate_count\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"use_gpu\": True,\n",
    "    \"sample_for_expansion\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd993546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:39:19,993 - INFO - Loading collection data from: data/subtask4b_collection_data.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running evaluation on dev set ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 23:39:20,553 - INFO - Loading query data from: data/subtask4b_query_tweets_dev.tsv\n",
      "2025-05-09 23:39:20,557 - INFO - Collection size: 7718\n",
      "2025-05-09 23:39:20,558 - INFO - Query set size: 1400\n",
      "2025-05-09 23:39:20,558 - INFO - \n",
      "=== Running langchain_rag ===\n",
      "Creating documents: 100%|██████████| 7718/7718 [00:00<00:00, 37911.80it/s]\n",
      "2025-05-09 23:39:20,773 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "def run_test_predictions():\n",
    "    \"\"\"\n",
    "    Run predictions on the test set for final submission\n",
    "    \"\"\"\n",
    "    # Run evaluation (prediction only) on test set\n",
    "    results = evaluate_models(\n",
    "        collection_path=\"data/subtask4b_collection_data.pkl\",\n",
    "        query_path=\"data/subtask4b_query_tweets_test.tsv\",\n",
    "        models_to_run=[\"langchain_rag\"],\n",
    "        output_dir=\"results\",\n",
    "        collection_columns=[\"title\", \"abstract\", \"authors\"],\n",
    "        top_k=5,\n",
    "        sample_size=None,             # use full test set\n",
    "        collection_sample_size=None,  # use full collection\n",
    "        mrr_k=[1, 5, 10],             # ignored for test set\n",
    "        **DEFAULT_CONFIG\n",
    "    )\n",
    "\n",
    "    print(\"\\nPredictions completed successfully!\")\n",
    "    print(\"Check the 'results' directory for your submission file.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_dev_evaluation():\n",
    "    \"\"\"\n",
    "    Run evaluation on the dev set to check model performance\n",
    "    \"\"\"\n",
    "    # Run evaluation on dev set\n",
    "    results = evaluate_models(\n",
    "        collection_path=\"data/subtask4b_collection_data.pkl\",\n",
    "        query_path=\"data/subtask4b_query_tweets_dev.tsv\",\n",
    "        models_to_run=[\"langchain_rag\"],\n",
    "        output_dir=\"results\",\n",
    "        collection_columns=[\"title\", \"abstract\", \"authors\"],\n",
    "        top_k=5,\n",
    "        sample_size=None,             # full dev set\n",
    "        collection_sample_size=None,  # full collection\n",
    "        mrr_k=[1, 5, 10],\n",
    "        **DEFAULT_CONFIG\n",
    "    )\n",
    "\n",
    "    # Print and analyze results\n",
    "    print(\"\\n=== Dev Set Results ===\")\n",
    "    for model, scores in results.items():\n",
    "        print(f\"{model} MRR@1: {scores.get(1, 'N/A')}\")\n",
    "        print(f\"{model} MRR@5: {scores.get(5, 'N/A')}\")\n",
    "        print(f\"{model} MRR@10: {scores.get(10, 'N/A')}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    # Determine best model (by MRR@5)\n",
    "    if results:\n",
    "        best_model = max(results.items(), key=lambda x: x[1].get(5, 0))[0]\n",
    "        print(f\"\\nBest model: {best_model}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Running evaluation on dev set ===\")\n",
    "    dev_results = run_dev_evaluation()\n",
    "\n",
    "    print(\"\\n=== Running predictions on test set ===\")\n",
    "    test_results = run_test_predictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
