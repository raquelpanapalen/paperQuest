{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Claim Retrieval: Complete Evaluation Workflow\n",
    "\n",
    "This notebook:\n",
    "1. Evaluates all models on the dev set\n",
    "2. Identifies the best performing model\n",
    "3. Generates test set predictions using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from evaluator import evaluate_models\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Full Configuration for All Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82322f64",
   "metadata": {},
   "source": [
    "Available Models:\n",
    "```\n",
    "        # Traditional methods\n",
    "        'bm25',              # Basic BM25\n",
    "        'tfidf',             # Basic TF-IDF\n",
    "        \n",
    "        # Representation learning  \n",
    "        'custom_retriever',          # Dense semantic embeddings\n",
    "        'vector_store',      # Vector database retrieval\n",
    "        \n",
    "        # Reranking methods\n",
    "        'bm25_reranker',     # BM25 + neural reranking\n",
    "        'tfidf_reranker',    # TF-IDF + neural reranking\n",
    "        'custom_retriever_reranker', # Two-stage with neural reranking\n",
    "        'vector_store_reranker', # Vector store + reranking\n",
    "        \n",
    "        # Hybrid methods\n",
    "        'multi_stage_hybrid',      # retrieve, fuse, rerank\n",
    "        \n",
    "        # Query expansion\n",
    "        'query_expansion'    # LLM-enhanced queries\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d35ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_CONFIG = {\n",
    "    # Data paths\n",
    "    'collection_path': 'data/subtask4b_collection_data.pkl',\n",
    "    'query_path': 'data/subtask4b_query_tweets_dev.tsv',\n",
    "    \n",
    "    # Available model categories and names:\n",
    "    'models': [\n",
    "        # Traditional methods\n",
    "        'bm25',              # Basic BM25\n",
    "        #'tfidf',             # Basic TF-IDF\n",
    "        \n",
    "        # Representation learning  \n",
    "        #'custom_retriever',          # Dense semantic embeddings\n",
    "        #'vector_store',      # Vector database retrieval\n",
    "        \n",
    "        # Reranking methods\n",
    "        'bm25_reranker',     # BM25 + neural reranking\n",
    "        #'tfidf_reranker',    # TF-IDF + neural reranking\n",
    "        #'custom_retriever_reranker', # Two-stage with neural reranking\n",
    "        #'vector_store_reranker', # Vector store + reranking\n",
    "        \n",
    "        # Hybrid methods\n",
    "        #'multi_stage_hybrid',      # retrieve, fuse, rerank\n",
    "        \n",
    "        # Query expansion\n",
    "        #'query_expansion'    # LLM-enhanced queries\n",
    "    ], \n",
    "    \n",
    "    # Output settings\n",
    "    'output_dir': f'results/eval_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    \n",
    "    # Model configuration\n",
    "    'top_k': 5,\n",
    "    'mrr_k': [1, 5, 10],\n",
    "    'collection_columns': ['title', 'abstract', 'authors'],\n",
    "    \n",
    "    # Model-specific settings\n",
    "    'embedding_model': 'sentence-transformers/allenai-specter',\n",
    "    'vectordb_model': 'all-minilm', # 'nomic-embed-text', #  'all-minilm', // momic performs worse\n",
    "    'reranker_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2', # 'BAAI/bge-reranker-base', #\n",
    "    \n",
    "    # Hybrid settings\n",
    "    'rrf_k': 60,\n",
    "    'sparse_weight': 0.6,\n",
    "    \n",
    "    # Performance settings\n",
    "    'candidate_count': 75,\n",
    "    'batch_size': 32,\n",
    "    'reranker_batch_size': 8,\n",
    "    'use_gpu': True,\n",
    "\n",
    "    # Sampling for testing\n",
    "    'sample_size': None, # tweets\n",
    "    'collection_sample_size': None, # abstracts\n",
    "    \n",
    "    'cache_dir': 'cache',\n",
    "    'show_progress': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate All Models on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 00:08:57,404 - INFO - Running bm25...\n",
      "Processing bm25: 100%|██████████| 1400/1400 [01:12<00:00, 19.24it/s]\n",
      "2025-05-30 00:10:11,330 - INFO - bm25 MRR@5: 0.5590\n",
      "2025-05-30 00:10:11,334 - INFO - Running bm25_reranker...\n",
      "Processing bm25_reranker:  23%|██▎       | 328/1400 [03:35<11:44,  1.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dev_results = \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEV_CONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluation completed! Results saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdev_results[\u001b[33m'\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git2/priv_cop/subtask4b/evaluator.py:181\u001b[39m, in \u001b[36mevaluate_models\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03mSimple function interface for evaluating models\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \u001b[33;03m    Results dictionary\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    180\u001b[39m evaluator = SimpleEvaluator()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git2/priv_cop/subtask4b/evaluator.py:86\u001b[39m, in \u001b[36mSimpleEvaluator.evaluate\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query_text \u001b[38;5;129;01min\u001b[39;00m tqdm(query_df[\u001b[33m'\u001b[39m\u001b[33mtweet_text\u001b[39m\u001b[33m'\u001b[39m].tolist(), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m         preds.append(pred)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git2/priv_cop/subtask4b/models/reranking_methods/bm25_reranker.py:88\u001b[39m, in \u001b[36mBM25Reranker.retrieve\u001b[39m\u001b[34m(self, query_text, top_k)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Rerank\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreranker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     scored_candidates = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(candidates, scores))\n\u001b[32m     90\u001b[39m     scored_candidates.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git2/priv_cop/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:338\u001b[39m, in \u001b[36mCrossEncoder.predict\u001b[39m\u001b[34m(self, sentences, batch_size, show_progress_bar, num_workers, activation_fct, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28mself\u001b[39m.model.to(\u001b[38;5;28mself\u001b[39m._target_device)\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_predictions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_predictions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git2/priv_cop/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git2/priv_cop/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git2/priv_cop/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/git2/priv_cop/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:145\u001b[39m, in \u001b[36mCrossEncoder.smart_batching_collate_text_only\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    140\u001b[39m tokenized = \u001b[38;5;28mself\u001b[39m.tokenizer(\n\u001b[32m    141\u001b[39m     *texts, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[33m\"\u001b[39m\u001b[33mlongest_first\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, max_length=\u001b[38;5;28mself\u001b[39m.max_length\n\u001b[32m    142\u001b[39m )\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m tokenized:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     tokenized[name] = \u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_target_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "dev_results = evaluate_models(DEV_CONFIG)\n",
    "\n",
    "print(f\"\\nEvaluation completed! Results saved to: {dev_results['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Display Results and Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for all models\n",
    "print(\"=== Dev Set Evaluation Results ===\")\n",
    "\n",
    "if dev_results['metrics']:\n",
    "    for model_name, metrics in dev_results['metrics'].items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  MRR@1: {metrics[1]:.4f}\")\n",
    "        print(f\"  MRR@5: {metrics[5]:.4f}\")\n",
    "        print(f\"  MRR@10: {metrics[10]:.4f}\")\n",
    "    \n",
    "    # Find best model based on MRR@5\n",
    "    best_model = max(dev_results['metrics'].items(), key=lambda x: x[1][5])[0]\n",
    "    best_score = dev_results['metrics'][best_model][5]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(f\"Best model: {best_model} (MRR@5: {best_score:.4f})\")\n",
    "    print(\"=\" * 30)\n",
    "else:\n",
    "    print(\"No evaluation metrics available (test set mode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05116ef1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Configuration for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant configuration parameters for the best model\n",
    "print(f\"Creating test configuration for model: {best_model}\")\n",
    "\n",
    "# Start with the base configuration\n",
    "TEST_CONFIG = {\n",
    "    # Update paths for test set\n",
    "    'collection_path': DEV_CONFIG['collection_path'],\n",
    "    'query_path': 'data/subtask4b_query_tweets_test.tsv',  # Test set for final submission\n",
    "\n",
    "    # Use only the best model\n",
    "    'models': [best_model],\n",
    "    \n",
    "    # New output directory for test predictions\n",
    "    'output_dir': f'results/test_{best_model}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    \n",
    "    # Copy relevant settings from dev config\n",
    "    'top_k': DEV_CONFIG['top_k'],\n",
    "    'collection_columns': DEV_CONFIG['collection_columns'],\n",
    "    'cache_dir': DEV_CONFIG['cache_dir'],\n",
    "    'batch_size': DEV_CONFIG['batch_size'],\n",
    "    'use_gpu': DEV_CONFIG['use_gpu'],\n",
    "}\n",
    "\n",
    "# Add model-specific settings based on the best model type\n",
    "if 'langchain' in best_model:\n",
    "    TEST_CONFIG['langchain_embedding'] = DEV_CONFIG['vectordb_model']\n",
    "    TEST_CONFIG['candidate_count'] = DEV_CONFIG['candidate_count']\n",
    "    \n",
    "    if 'reranker' in best_model:\n",
    "        TEST_CONFIG['reranker_model'] = DEV_CONFIG['reranker_model']\n",
    "        TEST_CONFIG['reranker_batch_size'] = DEV_CONFIG['reranker_batch_size']\n",
    "    \n",
    "    if 'query_expansion' in best_model:\n",
    "        TEST_CONFIG['sample_for_expansion'] = DEV_CONFIG['sample_for_expansion']\n",
    "\n",
    "elif best_model == 'dense':\n",
    "    TEST_CONFIG['embedding_model'] = DEV_CONFIG['embedding_model']\n",
    "\n",
    "elif best_model == 'neural_rerank':\n",
    "    TEST_CONFIG['reranker_model'] = DEV_CONFIG['reranker_model']\n",
    "    TEST_CONFIG['reranker_batch_size'] = DEV_CONFIG['reranker_batch_size']\n",
    "    TEST_CONFIG['candidate_count'] = DEV_CONFIG['candidate_count']\n",
    "\n",
    "print(\"\\nTest configuration created:\")\n",
    "print(json.dumps(TEST_CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating test predictions using {best_model}...\")\n",
    "\n",
    "# Run prediction on test set\n",
    "test_results = evaluate_models(TEST_CONFIG)\n",
    "\n",
    "print(\"\\nTest predictions completed!\")\n",
    "print(f\"Prediction file saved to: {test_results['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Complete Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary of the entire evaluation process\n",
    "summary = {\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'dev_results': {\n",
    "        'metrics': dev_results['metrics'],\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'output_dir': dev_results['output_dir']\n",
    "    },\n",
    "    'test_results': {\n",
    "        'model_used': best_model,\n",
    "        'output_dir': test_results['output_dir'],\n",
    "        'config': TEST_CONFIG\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary to file\n",
    "summary_file = f'results/evaluation_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nComplete evaluation summary saved to: {summary_file}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n=== EVALUATION COMPLETE ===\")\n",
    "print(f\"1. Evaluated {len(dev_results['metrics'])} models on dev set\")\n",
    "print(f\"2. Best model: {best_model} (MRR@5: {best_score:.4f})\")\n",
    "print(\"3. Test predictions generated and saved\")\n",
    "print(\"\\nAll results are in the 'results' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
