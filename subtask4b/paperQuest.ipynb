{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Claim Retrieval: Complete Evaluation Workflow\n",
    "\n",
    "This notebook:\n",
    "1. Evaluates all models on the dev set\n",
    "2. Identifies the best performing model\n",
    "3. Generates test set predictions using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from evaluator import evaluate_models\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Full Configuration for All Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82322f64",
   "metadata": {},
   "source": [
    "Available Models:\n",
    "```\n",
    "    'bm25': BM25Retriever,\n",
    "    'enhanced_bm25': EnhancedBM25Retriever,\n",
    "    'tfidf': TfidfRetriever,\n",
    "\n",
    "    'dense': DenseRetriever,\n",
    "    'neural_rerank': NeuralReranker,\n",
    "    'hybrid_rerank': HybridNeuralReranker,\n",
    "\n",
    "    'langchain_rag': LangChainRAGRetriever,\n",
    "    'langchain_reranker': LangChainRerankerRetriever,\n",
    "    'langchain_query_expansion': LangChainQueryExpansionRetriever,\n",
    "\n",
    "    'hybrid_retriever': HybridRetriever\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_CONFIG = {\n",
    "    # Data paths\n",
    "    'collection_path': 'data/subtask4b_collection_data.pkl',\n",
    "    'query_path': 'data/subtask4b_query_tweets_dev.tsv',\n",
    "    \n",
    "    # Evaluate models           \n",
    "    'models': [\n",
    "               #'bm25', \n",
    "               #'enhanced_bm25', \n",
    "               #'tfidf', \n",
    "               'langchain_rag', \n",
    "               'langchain_reranker',\n",
    "               #'dense', \n",
    "               #'neural_rerank',\n",
    "               #'hybrid_rerank', \n",
    "               #'hybrid_retriever',\n",
    "               ], \n",
    "    \n",
    "    # Output directory with timestamp\n",
    "    'output_dir': f'results/dev_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    \n",
    "    # Evaluation settings\n",
    "    'top_k': 5,\n",
    "    'mrr_k': [1, 5, 10],\n",
    "    'collection_columns': ['title', 'abstract', 'authors', 'journal', 'publish_time'],\n",
    "    \n",
    "    # Existing model settings\n",
    "    'embedding_model': 'sentence-transformers/allenai-specter',\n",
    "    'vectordb_model': 'nomic-embed-text', # 'all-minilm',\n",
    "    'reranker_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2', # 'BAAI/bge-reranker-base', #\n",
    "    \n",
    "    # Hybrid retrieval settings\n",
    "    'rrf_k': 60,  # Reciprocal Rank Fusion constant\n",
    "    'sparse_weight': 0.6,  # Weight for sparse retrieval in hybrid\n",
    "    \n",
    "    # Performance settings\n",
    "    'candidate_count': 75,\n",
    "    'batch_size': 32,\n",
    "    'reranker_batch_size': 8,\n",
    "    'use_gpu': True,\n",
    "\n",
    "    # Data sampling (None = use full datasets)\n",
    "    'sample_size': 150,\n",
    "    'collection_sample_size': 1000,\n",
    "    \n",
    "    # Cache directory\n",
    "    'cache_dir': 'cache',\n",
    "\n",
    "    'show_progress': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate All Models on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 13:50:58,478 - INFO - Running langchain_rag...\n",
      "Creating documents: 100%|██████████| 1000/1000 [00:00<00:00, 40926.43it/s]\n",
      "Processing langchain_rag:   0%|          | 0/150 [00:00<?, ?it/s]/Users/simonkerner/Desktop/git2/priv_cop/subtask4b/models/langchain_models.py:59: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = self.base_retriever.get_relevant_documents(query_text)[:top_k]\n",
      "Processing langchain_rag: 100%|██████████| 150/150 [00:02<00:00, 58.97it/s]\n",
      "2025-05-29 13:51:19,210 - INFO - langchain_rag MRR@5: 0.7014\n",
      "2025-05-29 13:51:19,212 - INFO - Running langchain_reranker...\n",
      "Creating documents: 100%|██████████| 1000/1000 [00:00<00:00, 34781.52it/s]\n",
      "Processing langchain_reranker: 100%|██████████| 150/150 [02:24<00:00,  1.04it/s]\n",
      "2025-05-29 13:54:08,329 - INFO - langchain_reranker MRR@5: 0.7351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed! Results saved to: results/dev_20250529_135056\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "dev_results = evaluate_models(DEV_CONFIG)\n",
    "\n",
    "print(f\"\\nEvaluation completed! Results saved to: {dev_results['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Display Results and Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for all models\n",
    "print(\"=== Dev Set Evaluation Results ===\")\n",
    "\n",
    "if dev_results['metrics']:\n",
    "    for model_name, metrics in dev_results['metrics'].items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  MRR@1: {metrics[1]:.4f}\")\n",
    "        print(f\"  MRR@5: {metrics[5]:.4f}\")\n",
    "        print(f\"  MRR@10: {metrics[10]:.4f}\")\n",
    "    \n",
    "    # Find best model based on MRR@5\n",
    "    best_model = max(dev_results['metrics'].items(), key=lambda x: x[1][5])[0]\n",
    "    best_score = dev_results['metrics'][best_model][5]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(f\"Best model: {best_model} (MRR@5: {best_score:.4f})\")\n",
    "    print(\"=\" * 30)\n",
    "else:\n",
    "    print(\"No evaluation metrics available (test set mode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05116ef1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Configuration for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant configuration parameters for the best model\n",
    "print(f\"Creating test configuration for model: {best_model}\")\n",
    "\n",
    "# Start with the base configuration\n",
    "TEST_CONFIG = {\n",
    "    # Update paths for test set\n",
    "    'collection_path': DEV_CONFIG['collection_path'],\n",
    "    'query_path': 'data/subtask4b_query_tweets_test.tsv',  # Test set for final submission\n",
    "\n",
    "    # Use only the best model\n",
    "    'models': [best_model],\n",
    "    \n",
    "    # New output directory for test predictions\n",
    "    'output_dir': f'results/test_{best_model}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    \n",
    "    # Copy relevant settings from dev config\n",
    "    'top_k': DEV_CONFIG['top_k'],\n",
    "    'collection_columns': DEV_CONFIG['collection_columns'],\n",
    "    'cache_dir': DEV_CONFIG['cache_dir'],\n",
    "    'batch_size': DEV_CONFIG['batch_size'],\n",
    "    'use_gpu': DEV_CONFIG['use_gpu'],\n",
    "}\n",
    "\n",
    "# Add model-specific settings based on the best model type\n",
    "if 'langchain' in best_model:\n",
    "    TEST_CONFIG['langchain_embedding'] = DEV_CONFIG['vectordb_model']\n",
    "    TEST_CONFIG['candidate_count'] = DEV_CONFIG['candidate_count']\n",
    "    \n",
    "    if 'reranker' in best_model:\n",
    "        TEST_CONFIG['reranker_model'] = DEV_CONFIG['reranker_model']\n",
    "        TEST_CONFIG['reranker_batch_size'] = DEV_CONFIG['reranker_batch_size']\n",
    "    \n",
    "    if 'query_expansion' in best_model:\n",
    "        TEST_CONFIG['sample_for_expansion'] = DEV_CONFIG['sample_for_expansion']\n",
    "\n",
    "elif best_model == 'dense':\n",
    "    TEST_CONFIG['embedding_model'] = DEV_CONFIG['embedding_model']\n",
    "\n",
    "elif best_model == 'neural_rerank':\n",
    "    TEST_CONFIG['reranker_model'] = DEV_CONFIG['reranker_model']\n",
    "    TEST_CONFIG['reranker_batch_size'] = DEV_CONFIG['reranker_batch_size']\n",
    "    TEST_CONFIG['candidate_count'] = DEV_CONFIG['candidate_count']\n",
    "\n",
    "print(\"\\nTest configuration created:\")\n",
    "print(json.dumps(TEST_CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating test predictions using {best_model}...\")\n",
    "\n",
    "# Run prediction on test set\n",
    "test_results = evaluate_models(TEST_CONFIG)\n",
    "\n",
    "print(\"\\nTest predictions completed!\")\n",
    "print(f\"Prediction file saved to: {test_results['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Complete Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary of the entire evaluation process\n",
    "summary = {\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'dev_results': {\n",
    "        'metrics': dev_results['metrics'],\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'output_dir': dev_results['output_dir']\n",
    "    },\n",
    "    'test_results': {\n",
    "        'model_used': best_model,\n",
    "        'output_dir': test_results['output_dir'],\n",
    "        'config': TEST_CONFIG\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary to file\n",
    "summary_file = f'results/evaluation_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nComplete evaluation summary saved to: {summary_file}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n=== EVALUATION COMPLETE ===\")\n",
    "print(f\"1. Evaluated {len(dev_results['metrics'])} models on dev set\")\n",
    "print(f\"2. Best model: {best_model} (MRR@5: {best_score:.4f})\")\n",
    "print(\"3. Test predictions generated and saved\")\n",
    "print(\"\\nAll results are in the 'results' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
